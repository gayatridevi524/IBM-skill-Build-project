# -*- coding: utf-8 -*-
"""Employee Burnout prediction .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xrd_ZBcJ3Q0uqaL5G0KyOzVzOuG-AqqS

**Importing Libraries**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

import warnings
warnings.filterwarnings('ignore') #ignoring warning messages during code execution

# Loading files from the colab
from google.colab import files
uploaded = files.upload()

"""**Loading Dataset**"""

#reads the csv file
data = pd.read_csv("employee_burnout.csv")
data  #prints the loaded dataset

#returns the shape of the dataset
data.shape

#returns the structure and type of data stored
data.info()

# converting string to datetime data type
data["Date of Joining"]=pd.to_datetime(data["Date of Joining"])

# now the data type of the DOJ column changed to datetime
data.info()

#returns the first 5 rows of the data
data.head()

# returns the columns present in the data
data.columns

# calculates the number of null values in each column of the data
data.isna().sum()

# checks for the duplicated values
data.duplicated().sum()

# calculates the mean,std,min,max,count of every attribute
data.describe()

# shows the unique values
for col in data.columns:
    unique_values = data[col].unique()#retrives unique values in the current column of data
    value_counts = data[col].value_counts()# calculates the count of each unique value in the current column using value_counts function
    print(f"\n\n{unique_values}")
    print(f"\n{value_counts}\n\n")

# Droping out the irrelevant column as it was no longer required
data=data.drop(["Employee ID"],axis=1)

# checks for the skewness of the attributes
intFloatdata=data.select_dtypes([np.int,np.float])
for i,col in enumerate(intFloatdata.columns):
  if(intFloatdata[col].skew()>=0.1):
    print("\n",col,"feature is positively skewed and value is: ",intFloatdata[col].skew())
  elif(intFloatdata[col].skew()<=-0.1):
    print("\n",col,"feature is negatively skewed and value is: ",intFloatdata[col].skew())
  else:
    print("\n",col,"feature is normally distributed and value is: ",intFloatdata[col].skew())

# Replaces null values with mean values
data["Resource Allocation"].fillna(data["Resource Allocation"].mean(),inplace=True)
data["Mental Fatigue Score"].fillna(data["Mental Fatigue Score"].mean(),inplace=True)
data["Burn Rate"].fillna(data["Burn Rate"].mean(),inplace=True)

# again check for the null values
data.isna().sum()

# calculates the correlation between the features of data -1 indicates negative,0 indicates none,+1 indicates positive
data.corr()

"""**Data Visualization**"""

# Plotting heatmap to check the correlation
corr=data.corr()
sns.set(rc={'figure.figsize':(14,12)})
fig=px.imshow(corr,text_auto=True,aspect='auto')
fig.show()

# Count plot distribution based on Gender
plt.figure(figsize=(10,8))
sns.countplot(x="Gender",data=data,palette="magma")
plt.title("plot distribution for gender")
plt.show()

#count plot distribution of company type
plt.figure(figsize=(10,8))
sns.countplot(x="Company Type",data=data,palette="Spectral")
plt.title("plot distribution of Company Type")
plt.show()

#count plot distribution of "WFH setup available"
plt.figure(figsize=(10,8))
sns.countplot(x="WFH Setup Available",data=data,palette="dark:salmon_r")
plt.title("plot distribution of WFH Setup Available")
plt.show()

#count plot distribution of attributes with the help of histogram
burn_st=data.loc[:,"Date of Joining":"Burn Rate"]
burn_st=burn_st.select_dtypes([int,float])
for i,col in enumerate(burn_st.columns):
  fig=px.histogram(burn_st,x=col,title="plot distribution of "+col,color_discrete_sequence=["indianred"])
  fig.update_layout(bargap=0.2)
  fig.show()

#plot distribution of the Burn rate on the basis of Designation
fig=px.line(data,y="Burn Rate",color="Designation",title="Burn Rate on the basis of Designation",color_discrete_sequence=px.colors.qualitative.Pastel1)
fig.update_layout(bargap=0.1)
fig.show()

#plot distribution of Burn Rate on the basis of Gender
fig=px.line(data,y="Burn Rate",color="Gender",title="Burn Rate on the basis of Gender",color_discrete_sequence=px.colors.qualitative.Pastel1)
fig.update_layout(bargap=0.1)
fig.show()

#plot distribution of mental fatigue score on the basis of Designation
fig=px.line(data,y="Mental Fatigue Score",color="Designation",title="Mental fatigue vs Designation",color_discrete_sequence=px.colors.qualitative.Pastel1)
fig.update_layout(bargap=0.2)
fig.show()

#plot distribution of "Designation vs mental fatigue" as per Company type,Burn rate and Gender
sns.relplot(
    data=data,x="Designation",y="Mental Fatigue Score",col="Company Type",
    hue="Company Type",size="Burn Rate",style="Gender",
    palette=['g','r'],sizes=(50,200)
)

"""**Label Encoding**"""

# Importing Label Encoder and assigning it to a new variable
from sklearn import preprocessing
label_encode=preprocessing.LabelEncoder()

data['GenderLabel']=label_encode.fit_transform(data["Gender"].values)
data["Company_TypeLabel"]=label_encode.fit_transform(data["Company Type"].values)
data['WFH_Setup_AvailableLabel']=label_encode.fit_transform(data["WFH Setup Available"].values)

#check assigned values for Gender
gn=data.groupby('Gender')
gn=gn['GenderLabel']
gn.first()

#ckeck assigned values for Company Type
ct=data.groupby('Company Type')
ct=ct["Company_TypeLabel"]
ct.first()

#checks assigned value for WFH setup available
wsa=data.groupby('WFH Setup Available')
wsa=wsa['WFH_Setup_AvailableLabel']
wsa.first()

# returns the last 10 rows of the data
data.tail(10)

"""**Feature Selection**"""

# Feature Selection
columns=['Designation','Resource Allocation','Mental Fatigue Score',
         'GenderLabel','Company_TypeLabel','WFH_Setup_AvailableLabel']
X=data[columns] # independent variables
y=data["Burn Rate"] # dependent variable

print(X)

print(y)

"""**Principal Component Analysis (PCA)**"""

from sklearn.decomposition import PCA

pca=PCA(0.95)
X_pca=pca.fit_transform(X)

print("PCA shape of X is:",X_pca.shape,"and original shape is:",X.shape)
print("% of importance of selected features is:",pca.explained_variance_ratio_)
print("The number of features selected through PCA is:",pca.n_components_)

"""**Data Splitting**"""

# splits the dataset into train and test sets
from sklearn.model_selection import train_test_split
X_train_pca,X_test,y_train,y_test=train_test_split(X_pca,y,test_size=0.25,random_state=10)

# print the shape of the splitting data
print(X_train_pca.shape,X_test.shape,y_train.shape,y_test.shape)

"""**Model Implementation**"""

from sklearn.metrics import r2_score

#Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor

rf_model=RandomForestRegressor()
rf_model.fit(X_train_pca,y_train)

train_pred_rf=rf_model.predict(X_train_pca)
train_r2=r2_score(y_train,train_pred_rf)
test_pred_rf=rf_model.predict(X_test)
test_r2=r2_score(y_test,test_pred_rf)

#Accuracy Score
print("Accuracy score of train data:"+str(round(100*train_r2,4))+"%")
print("Accuracy score of test data:"+str(round(100*test_r2,4))+"%")

#AdaBoost Regressor
from sklearn.ensemble import AdaBoostRegressor
abr_model=AdaBoostRegressor()
abr_model.fit(X_train_pca,y_train)

train_pred_adboost=abr_model.predict(X_train_pca)
train_r2=r2_score(y_train,train_pred_adboost)
test_pred_adaboost=abr_model.predict(X_test)
test_r2=r2_score(y_test,test_pred_adaboost)

#Accuracy Score
print("Accuracy score of train data:"+str(round(100*train_r2,4))+"%")
print("Accuracy score of test data:"+str(round(100*test_r2,4))+"%")